{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Streaming Twitter Data_\n",
    "\n",
    "When people traditionally think of data analysis, one of the first steps tends to be reading in data, via a CSV file or by querying a database for example, after which they can then explore it. This approach works fine when you analyzing historical data (e.g. what products a customer at an store has bought and is most likely to purchase or the effects of a particular advertising campaign on customers purchasing patterns). \n",
    "\n",
    "However, what if we want to explore social media data? While historical data can provide some value in this realm, we're leaving a lot on the table if we don't consider the continuous stream of data being generated every second of every day on platforms like Twitter, which we'll be focusing on today. Especially as it pertains to coronavirus/COVID-19, information is continually changing and being updated to reflect this change, so it is essential that we be able to gather streaming data. This will help us stay up-to-date with current trends, and to ensure that our product does not grow stale. \n",
    "\n",
    "That being said, how do you access real-time Twitter data, and more specifically, Tweets related to the coronavirus? You use [Twitter's API](https://developer.twitter.com/en) which gives us the ability to [filter Tweets in realtime](https://developer.twitter.com/en/docs/tweets/filter-realtime/overview). \n",
    "\n",
    "We can access all the Python tools we for streaming via the [tweepy](https://github.com/tweepy/tweepy) library, which can be installed via pip using the following command: \n",
    "- `pip install tweepy`\n",
    "\n",
    "Once that's installed we're ready to start! The first step will be to check on the speed of the streaming data. In other words, how fast are we able to stream tweets in? To test this, we'll begin by setting up a very basic tweepy [`StreamListener`](http://docs.tweepy.org/en/latest/streaming_how_to.html) to see how quickly we can gather 1,000 tweets, which we'll also output as a CSV so we can check the content of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler, Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tools we're going to put together a custom `StreamListener` that'll filter tweets according to the terms `covid19` & `coronavirus`, which will then grab a few basic data points associated with the Tweet, such as the text, when it was created, and the username of the user who tweeted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a streamer object\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api = None):\n",
    "        self.api = api\n",
    "        self.num_tweets = 0\n",
    "        self.filename = \"data_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".csv\"\n",
    "        csvfile = open(self.filename, \"w\")\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        \n",
    "        # write a single row with headers of the columns\n",
    "        csvwriter.writerow([\n",
    "            \"created_at\", \"user_id\", \"user_screenname\", \"tweet_id\", \"text\"\n",
    "        ])\n",
    "    \n",
    "    # when a tweet appears\n",
    "    def on_status(self, status):\n",
    "        csvfile = open(self.filename, \"a\")\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        \n",
    "        # if the tweet is not a retweet\n",
    "        if not \"RT @\" in status.text:\n",
    "            try:\n",
    "                self.num_tweets += 1\n",
    "                if self.num_tweets <= 1000:\n",
    "                    csvwriter.writerow([\n",
    "                        status.created_at, status.user.id, status.user.screen_name, status.id, status.text\n",
    "                    ])\n",
    "                    if self.num_tweets % 100 == 0:\n",
    "                        print(\"Number of Tweets gathered: \", self.num_tweets)\n",
    "                else:\n",
    "                    # once we've gathered 1,000 tweets stop the stream\n",
    "                    return False\n",
    "            # if some error occurs\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass # print error and continue\n",
    "            \n",
    "        csvfile.close()\n",
    "        return\n",
    "    \n",
    "    # when an error occurs\n",
    "    def on_error(self, status_code):\n",
    "        print(\"Encountered error with status code:\", status_code)\n",
    "        \n",
    "        # if error code for bad credentials, end the stream\n",
    "        if status_code == 401:\n",
    "            return False\n",
    "        \n",
    "    # when a deleted tweet appears\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "    \n",
    "    # when reach the rate limit\n",
    "    def on_limit(self, track):\n",
    "        # continue mining tweets\n",
    "        return True\n",
    "    \n",
    "    # when timed out\n",
    "    def on_timeout(self):\n",
    "        print(sys.stderr, \"Timeout...\")\n",
    "        time.sleep(10)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have our `StreamListener`, which will gather 1,000 Tweets in real-time, and append each record to a CSV file. Before we can use it however, we need to make an additional function, a wrapper, which will be used to set up our connection to the API and subsequently begin the streaming process. \n",
    "\n",
    "Now Twitter doesn't just let anybody stream Tweets; you have to first create a developer account which will then give you the ability to create an app. Once the app has been approved, you receive four keys and tokens that then give you the ability to access Twitter data. This part of the set-up process is outside of the scope of this notebook as having already created an app, I have a set of keys/tokens available to use for our `StreamListener`. These keys/tokens ARE MEANT TO BE KEPT PRIVATE, as making them publicly available presents the opportunity for anybody to then be able to read (and potentially write to, depending on permissions) your Twitter!\n",
    "\n",
    "To be able to iterate quickly, I will be keeping these keys & tokens in a seperate Python script named `twitter_keys_tokens.py` that will never be uploaded to GitHub. Luckily accessing the data in this script is straightforward, as we can simply import it similar to how we would import `pandas` or `numpy`. After this, we can go ahead and create our wrapper function, which, given a list of strings, will return Tweets containing those strings. In our case, we'll pass in two terms: `covid19` and `coronavirus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_keys_tokens import keys_tokens\n",
    "\n",
    "def start_mining(queries):\n",
    "    # variables that contain credentials to access Twitter API\n",
    "    consumer_key = keys_tokens[\"API_KEY\"]\n",
    "    consumer_secret = keys_tokens[\"API_SECRET\"]\n",
    "    access_token = keys_tokens[\"ACCESS_TOKEN\"]\n",
    "    access_secret = keys_tokens[\"ACCESS_SECRET\"]\n",
    "    \n",
    "    # create a listener based on class above\n",
    "    listener = StdOutListener()\n",
    "    \n",
    "    # create authorization info\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    \n",
    "    # create Stream object\n",
    "    stream = Stream(auth, listener)\n",
    "    \n",
    "    # run the stream object, searching for tweets according to search terms and in English\n",
    "    stream.filter(track=queries, languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Run the Stream Miner_\n",
    "\n",
    "Now we have everything that we need! So what we'll do next is run `start_mining` with two search terms: `covid19`, and `coronavirus`. Additionally, we'll use the `%%time` magic command to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets gathered:  100\n",
      "Number of Tweets gathered:  200\n",
      "Number of Tweets gathered:  300\n",
      "Number of Tweets gathered:  400\n",
      "Number of Tweets gathered:  500\n",
      "Number of Tweets gathered:  600\n",
      "Number of Tweets gathered:  700\n",
      "Number of Tweets gathered:  800\n",
      "Number of Tweets gathered:  900\n",
      "Number of Tweets gathered:  1000\n",
      "CPU times: user 3.34 s, sys: 489 ms, total: 3.83 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start_mining([\"covid19, coronavirus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it took us `1min 46s` to gather 1,000 tweets with these particular methods. Let's check the CSV file that was generated to make sure the data that we got was indeed what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   created_at       1000 non-null   object\n",
      " 1   user_id          1000 non-null   int64 \n",
      " 2   user_screenname  1000 non-null   object\n",
      " 3   tweet_id         1000 non-null   int64 \n",
      " 4   text             1000 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_20200406-233602.csv\")\n",
    "\n",
    "# get info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_screenname</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-06 23:35:57</td>\n",
       "      <td>502114278</td>\n",
       "      <td>Frazzle_Rocks</td>\n",
       "      <td>1247307085899075592</td>\n",
       "      <td>What the fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-06 23:35:57</td>\n",
       "      <td>124022302</td>\n",
       "      <td>Colorado_Right</td>\n",
       "      <td>1247307086230212609</td>\n",
       "      <td>There still appears to be ZERO actual inertia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-06 23:35:57</td>\n",
       "      <td>1049590943840722944</td>\n",
       "      <td>saturnohes</td>\n",
       "      <td>1247307086305923079</td>\n",
       "      <td>coronavirus i want that shit to be gONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-06 23:35:57</td>\n",
       "      <td>1145584128076800000</td>\n",
       "      <td>lastboyalive</td>\n",
       "      <td>1247307086389825546</td>\n",
       "      <td>The world is going to be an entirely different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-06 23:35:57</td>\n",
       "      <td>813345856275574784</td>\n",
       "      <td>JohnTitor33621</td>\n",
       "      <td>1247307086440083457</td>\n",
       "      <td>The #vaccine may be legit, it may save lives, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at              user_id user_screenname  \\\n",
       "0  2020-04-06 23:35:57            502114278   Frazzle_Rocks   \n",
       "1  2020-04-06 23:35:57            124022302  Colorado_Right   \n",
       "2  2020-04-06 23:35:57  1049590943840722944      saturnohes   \n",
       "3  2020-04-06 23:35:57  1145584128076800000    lastboyalive   \n",
       "4  2020-04-06 23:35:57   813345856275574784  JohnTitor33621   \n",
       "\n",
       "              tweet_id                                               text  \n",
       "0  1247307085899075592                                      What the fuck  \n",
       "1  1247307086230212609  There still appears to be ZERO actual inertia ...  \n",
       "2  1247307086305923079            coronavirus i want that shit to be gONE  \n",
       "3  1247307086389825546  The world is going to be an entirely different...  \n",
       "4  1247307086440083457  The #vaccine may be legit, it may save lives, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the first five observations\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing looks critically wrong, but there are a few observations that I would like to make. The first being that the `created_at` column returns the date as a string, so in future iterations we will need to convert that to a datetime object. It is yet to be determined if it is better to do this up-front during the streaming or if we can convert it after the file is already generated (I am leaning towards the former, but this will depend on how much it impacts the speed). \n",
    "\n",
    "Next, in regards to the `user_id` column, even though the ID is returned correctly as an integer type, Twitter offers the ID in string format, which it recommends using over the integer version. Lastly, in the next iteration I hope to add a text-preprocessing step which will ensure the tweet is in a more appropriate format for analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
