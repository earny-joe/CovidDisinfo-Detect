{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Text Normalization and Tokenization_\n",
    "\n",
    "Machine learning algorithms technically don't work with text data, however, there is a workaround for this. By pre-processing the text, and then converting it into a numerical format (i.e. vectors of numbers), it is then in an appropriate format that can be fed into ML algorithms. But what does text pre-processing entail exactly?  \n",
    "\n",
    "This is where things get interesting. From a high-level, preprocessing removes as much noise as possible from the text data, that way an algorithm can more readily find any potential patterns. But determining what is noise and what is not is significantly impacted by the type of text. For example, you do not want to use the same text pre-processing techniques when you are analyzing Tweets versus when you are analyzing novels. Sure, there may be some overlap, but these two examples of text are significantly different not only in their function but in the text patterns that exist within them (after all, you won't see any emojis in Dostoyevsky's __The Brothers Karamazov__...)\n",
    "\n",
    "That being said there are some central core of text processing strategies that will help you get started:\n",
    "- __Lower casing__: by lowercasing all of the text data, it allows us to capture a word that may have multiple spellings due to miscellaneous uppercasing. For example, a text may include: `America`, `aMerica`, and `AMERICA`. Now we know these are all the same word, however, machines don't, they think these are three different words. To help our computer come to its senses, we lowercase all the text so it can then recognize three cases of `america`, instead of one case of three words. \n",
    "- __Stemming__: This means looking for the \"root\" of a word. There are plenty of words that have multiple inflections. Take the word `connect`; some of its inflections include: `connected`, `connection`, and `connects`. With stemming, we can crudely change the inflection words to the root word by chopping off their endings.\n",
    "- __Lemmatization__: Similar to stemming, in that its goal is to remove inflections, but it does it in a less crude way. It attempts to transform words to the actual root. Take the word `geese`, which is an inflection of `goose`. By using lemmatization, we can change it back into its original root (versus simply chopping off the letters at the end). \n",
    "- __Removing Stopwords__: When you are dealing with text, a lot of the words used actually provide no significant value. Examples include `a`, `this`, `and`, etc. What is the benefit of this? In theory, it allows us to keep only the important words. Lets take a look at the following sentence: `John is going to the store.` Now, let's remove `is`, `to`, and `the`: `John going store`. While it isn't grammatically correct, you still get the primary concept, that John is going to the store. While humans may think it's weird to read the above, this strategy has the potential to help a machine. \n",
    "- __Text Normalization__: Due to the character limits for Tweets, people will often use non-standard forms of words. One such example would be the use of `omg`, which stands for `oh my god`; another example would be the use of `2mrw` as a stand in for the word `tomorrow`. As I mentioned, this is pretty common pattern in social media text, so is a technique to seriously consider for this project. \n",
    "- __Text Enrichment / Augmentation__: Believe it or not, this strategy augments (i.e. adds) information that wasn't previously there before in hopes that can improve its predictive power. Sub-strategies could include things like part-of-speech tagging, or dependency parsing. \n",
    "\n",
    "With this background in mind, let's turn our attention to developing a function that will clean our Tweet data. This is v2 of the text preprocessing component of our pipeline; the first version can be found in the `8.1-je-text-normal-token` Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Our test Tweet_\n",
    "\n",
    "Below is `testtweet`, which contains an actual tweet from the data we've gathered, with a few modifications. I've added the following terms/items (i.e. these weren't in the original Tweet):\n",
    "\n",
    "- every newline character (e.g. `\\n`)\n",
    "- `Sars-Cov-2`\n",
    "- `corona virus`\n",
    "- the emoji (üßê)\n",
    "- `#coronavirus`\n",
    "- the YouTube link\n",
    "- random GitHub link from Ryan's tutorial repository\n",
    "\n",
    "The reason that I had this information is that each one represents a potential part that we must catch in our text preprocessing phase. We need to remove newline-characters because they provide next to no value; we need to account for emojis and replace them with special tokens to preserve that there was an emoji in the Tweet; we need to standardize various coronavirus/covid-related terms, both in regards to normal text and hashtags; and we need to address for the various types of URLs that are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtweet = \"\"\"Sars-Cov-2 @narendramodi Dear PM, Is this really happening?\\n\\nThe countrymen have to pay for COVID-19 \n",
    "corona virus #Covid19 Tests?  üßê Unbelievable! \\n #coronavirus pic.twitter.com/31nvInjcBZ \n",
    "https://www.youtube.com/watch?v=ig9yh8iVZWI\n",
    "https://github.com/rkingery/ml_tutorials/blob/master/notebooks/ml_with_text.ipynb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _v2 Text Preprocessing Functions_\n",
    "\n",
    "V1 of the text preprocessing component threw away or manipulated too much of the text data. Our primary focus with V2 is to retain as much information from the original Tweet as possible. With this in mind, lets turn our attention to the functions below.\n",
    "\n",
    "- `newline_remove`: replaces newline characters (e.g. `\\n`)\n",
    "- `replace_coronavirus`: standardizes coronavirus term within text to `coronavirus`\n",
    "- `coronavirus_hashtags`: any instances of `#coronavirus` are replaced with special token `xxhashcoronavirus`\n",
    "- `replace_covid`: standardizes COVID-19 within text to `covid19`\n",
    "- `covid_hashtags`: any instances of `#covid19` are replaced with special token `xxhashcovid19`\n",
    "- `sarscov2_replace`: accounts for any mention of `SARS-CoV-2` and standardizes to `sarscov2`\n",
    "- `emoji_replace`: replaces any emojis (e.g. üòâ) in a given text with a special token `xxemoji`\n",
    "- `twitterpic_replace`: if there is a link to a picutre in a format similar to `pic.twitter.com/`, we replace that substring with a special token `xxpictwit`\n",
    "- `youtube_replace`: similar to the above, but geared specifically to any YouTube links, replacing them with `xxyoutubeurl`\n",
    "- `url_replace`: again similar to the above, but geared specifically for any other miscellaneous URLs, replacing them with `xxurl`\n",
    "- `punctuation_replace`: ensures that punctuation has one space on either side of the character (makes it easier to pick out)\n",
    "- `clean_wrapper`: wrapper function that includes all the functions mentioned above\n",
    "\n",
    "Additionally, `clean_wrapper` has some extra functionality that gives us a little more flexbility to try and compare different preprocessing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sim_wrapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sim_wrapper.py\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize\n",
    "\n",
    "def newline_remove(text):\n",
    "    regex = re.compile(r'\\n+', re.I)\n",
    "    text = regex.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_coronavirus(text):\n",
    "    regex = re.compile(r'(corona[\\s]?virus)', re.I)\n",
    "    return regex.sub('coronavirus', text)\n",
    "\n",
    "\n",
    "def coronavirus_hashtags(text):\n",
    "    regex = re.compile(r'#(coronavirus)\\b', re.I)\n",
    "    return regex.sub('xxhashcoronavirus', text)\n",
    "\n",
    "\n",
    "def replace_covid(text):\n",
    "    regex = re.compile(r'(covid[-\\s_]?19)', re.I)\n",
    "    return regex.sub('covid19', text)\n",
    "\n",
    "\n",
    "def covid_hashtags(text):\n",
    "    regex = re.compile(r'#(covid[_-]?(19))', re.I)\n",
    "    return regex.sub('xxhashcovid19', text)\n",
    "\n",
    "\n",
    "def sarscov2_replace(text):\n",
    "    regex = re.compile(r'(sars[-]?cov[-]?2)', re.I)\n",
    "    return regex.sub(r'sarscov2', text)\n",
    "\n",
    "\n",
    "def emoji_replace(text):\n",
    "    # first demojize text\n",
    "    new_text = emoji.demojize(text, use_aliases=True)\n",
    "    regex = re.compile(r\"(:\\S+:)\", re.I)\n",
    "    return regex.sub(\" xxemoji \", new_text)\n",
    "\n",
    "\n",
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"xxpictwit\", text)\n",
    "\n",
    "\n",
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"xxyoutubeurl\", text)\n",
    "\n",
    "\n",
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"xxurl\", text)\n",
    "\n",
    "\n",
    "def punctuation_replace(text):\n",
    "    # put spaces between punctuation\n",
    "    PUNC = string.punctuation + '‚Ä¶‚Äì‚Äù‚Äú‚Äô'\n",
    "    punct = r\"[\" + re.escape(PUNC) + r\"]\"\n",
    "    text = re.sub(\"(?<! )(?=\" + punct + \")|(?<=\" + punct + \")(?! )\", r\" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\",'',text) # could replace with xxpunc\n",
    "    # remove any extra whitespace\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_wrapper(text, nltk_tokenize=False, punc_replace=False, preprocessor=False):\n",
    "    PUNC = string.punctuation + '‚Ä¶‚Äì‚Äù‚Äú‚Äô'\n",
    "    # removes newline characters from text\n",
    "    text = newline_remove(text)\n",
    "    # standardizes all instances of coronavirus in text\n",
    "    text = replace_coronavirus(text)\n",
    "    # replaces instances of #coronavirus with special token, xxhashcoronavirus\n",
    "    text = coronavirus_hashtags(text)\n",
    "    # standardizes all instances of covid19\n",
    "    text = replace_covid(text)\n",
    "    # replaces instances of #covid19 with special token, xxhashcovid19\n",
    "    text = covid_hashtags(text)\n",
    "    # standardizes SARS-Cov-2 to sarscov2\n",
    "    text = sarscov2_replace(text)\n",
    "    # removes hashtag characters\n",
    "    text = text.replace(r'#', '')\n",
    "    # removes @ character\n",
    "    text = text.replace(r'@', '')\n",
    "    # if preprocessor set to True, use preprocessor library to process tweet\n",
    "    if preprocessor == True:\n",
    "        p.set_options(p.OPT.NUMBER)\n",
    "        text = p.tokenize(text)\n",
    "        text = emoji_replace(text)\n",
    "        if punc_replace == True:\n",
    "            text = punctuation_replace(text)\n",
    "        text = \" \".join(word for word in text.split() if len(word) > 1)\n",
    "        return text.strip()\n",
    "    # replace emojies with special token xxemoji\n",
    "    text = emoji_replace(text)\n",
    "    # replace pic.twitter.com links with special token, xxpictwit\n",
    "    text = twitterpic_replace(text)\n",
    "    # replace YouTube links with special token, xxyoutubeurl\n",
    "    text = youtube_replace(text)\n",
    "    # replace other URLs with special token, xxurl\n",
    "    text = url_replace(text)\n",
    "    # if nltk_tokenize parameter True, then use regexp_tokenize from nltk library\n",
    "    if nltk_tokenize == True:\n",
    "        text = \" \".join(RegexpTokenizer('\\s+', gaps=True).tokenize(text))\n",
    "        text = \"\".join(char for char in text if char not in PUNC)\n",
    "    # if punc_replace set to True, replace all punctuations\n",
    "    if punc_replace == True:\n",
    "        text = punctuation_replace(text)\n",
    "    # remove any unnecessary whitespace\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
