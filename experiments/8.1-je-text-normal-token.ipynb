{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Text Normalization and Tokenization_\n",
    "\n",
    "Machine learning algorithms technically don't work with text data, however, there is a workaround for this. By pre-processing the text, and then converting it into a numerical format (i.e. vectors of numbers), it is then in an appropriate format that can be fed into ML algorithms. But what does text pre-processing entail exactly?  \n",
    "\n",
    "This is where things get interesting. From a high-level, preprocessing removes as much noise as possible from the text data, that way an algorithm can more readily find any potential patterns. But determining what is noise and what is not is significantly impacted by the type of text. For example, you do not want to use the same text pre-processing techniques when you are analyzing Tweets versus when you are analyzing novels. Sure, there may be some overlap, but these two examples of text are significantly different not only in their function but in the text patterns that exist within them (after all, you won't see any emojis in Dostoyevsky's __The Brothers Karamazov__...)\n",
    "\n",
    "That being said there are some central core of text processing strategies that will help you get started:\n",
    "- __Lower casing__: by lowercasing all of the text data, it allows us to capture a word that may have multiple spellings due to miscellaneous uppercasing. For example, a text may include: `America`, `aMerica`, and `AMERICA`. Now we know these are all the same word, however, machines don't, they think these are three different words. To help our computer come to its senses, we lowercase all the text so it can then recognize three cases of `america`, instead of one case of three words. \n",
    "- __Stemming__: This means looking for the \"root\" of a word. There are plenty of words that have multiple inflections. Take the word `connect`; some of its inflections include: `connected`, `connection`, and `connects`. With stemming, we can crudely change the inflection words to the root word by chopping off their endings.\n",
    "- __Lemmatization__: Similar to stemming, in that its goal is to remove inflections, but it does it in a less crude way. It attempts to transform words to the actual root. Take the word `geese`, which is an inflection of `goose`. By using lemmatization, we can change it back into its original root (versus simply chopping off the letters at the end). \n",
    "- __Removing Stopwords__: When you are dealing with text, a lot of the words used actually provide no significant value. Examples include `a`, `this`, `and`, etc. What is the benefit of this? In theory, it allows us to keep only the important words. Lets take a look at the following sentence: `John is going to the store.` Now, let's remove `is`, `to`, and `the`: `John going store`. While it isn't grammatically correct, you still get the primary concept, that John is going to the store. While humans may think it's weird to read the above, this strategy has the potential to help a machine. \n",
    "- __Text Normalization__: Due to the character limits for Tweets, people will often use non-standard forms of words. One such example would be the use of `omg`, which stands for `oh my god`; another example would be the use of `2mrw` as a stand in for the word `tomorrow`. As I mentioned, this is pretty common pattern in social media text, so is a technique to seriously consider for this project. \n",
    "- __Text Enrichment / Augmentation__: Believe it or not, this strategy augments (i.e. adds) information that wasn't previously there before in hopes that can improve its predictive power. Sub-strategies could include things like part-of-speech tagging, or dependency parsing. \n",
    "\n",
    "But before we can start doing this, let's load in all the functions that we previously created in the 8.0 notebook, to address issues unique to Tweets:\n",
    "- Links to Twitter pictures, YouTube videos, and other assorted URLs\n",
    "- Mentioning other Twitter users and hashtags\n",
    "- Emojis\n",
    "\n",
    "We'll create a wrapper function that performs all of these functions together versus having to run them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext line_profiler\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import emoji\n",
    "import spacy\n",
    "import string\n",
    "pd.options.display.max_columns = None\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.simplefilter(\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      " 19  processed_tweet     object\n",
      "dtypes: bool(1), int64(7), object(12)\n",
      "memory usage: 516.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"playground_data/covid19_0320_0324_updated_v2.pkl\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the functions below (except the wrapper) were taken from the 8.0 Text Preprocessing Notebook\n",
    "def newline_remove(text):\n",
    "    regex = re.compile(r\"\\n+\", re.I)\n",
    "    return regex.sub(\" \", text)\n",
    "\n",
    "def twitterpic_replace(text):\n",
    "    regex = re.compile(r\"pic.twitter.com/\\w+\", re.I)\n",
    "    return regex.sub(\"xxpictwit\", text)\n",
    "\n",
    "def youtube_replace(text):\n",
    "    regex = re.compile(r\"(https://youtu.be/(\\S+))|(https://www.youtube.(\\S+))\", re.I)\n",
    "    return regex.sub(\"xxyoutubeurl\", text)\n",
    "\n",
    "def url_replace(text):\n",
    "    regex = re.compile(r\"(?:http|ftp|https)://(\\S+)\", re.I)\n",
    "    return regex.sub(\"xxurl\", text)\n",
    "\n",
    "def usermention_replace(text):\n",
    "    regex = re.compile(r\"@([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxuser\", text)\n",
    "\n",
    "def hashtag_replace(text):\n",
    "    regex = re.compile(r\"#([^\\s:]+)+\", re.I)\n",
    "    return regex.sub(\"xxhashtag\", text)\n",
    "\n",
    "def emoji_replace(text):\n",
    "    # first demojize text\n",
    "    new_text = emoji.demojize(text, use_aliases=True)\n",
    "    regex = re.compile(r\"(:\\S+:)+\", re.I)\n",
    "    return regex.sub(\" xxemoji \", new_text)\n",
    "\n",
    "def unique_twitter_tokens(text):\n",
    "    text = newline_remove(text)\n",
    "    text = twitterpic_replace(text)\n",
    "    text = youtube_replace(text)\n",
    "    text = url_replace(text)\n",
    "    text = usermention_replace(text)\n",
    "    text = hashtag_replace(text)\n",
    "    text = emoji_replace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f82bd121f3f4e09bf456835bce46e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column for clean Tweet text\n",
    "df[\"clean_tweet\"] = df[\"tweet\"].progress_apply(unique_twitter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3364618 entries, 2020-03-24 23:59:59 to 2020-03-20 01:37:05\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   id                  int64 \n",
      " 1   conversation_id     int64 \n",
      " 2   user_id             int64 \n",
      " 3   username            object\n",
      " 4   name                object\n",
      " 5   tweet               object\n",
      " 6   mentions            object\n",
      " 7   urls                object\n",
      " 8   photos              object\n",
      " 9   replies_count       int64 \n",
      " 10  retweets_count      int64 \n",
      " 11  likes_count         int64 \n",
      " 12  hashtags            object\n",
      " 13  link                object\n",
      " 14  retweet             bool  \n",
      " 15  quote_url           object\n",
      " 16  video               int64 \n",
      " 17  reply_to_userids    object\n",
      " 18  reply_to_usernames  object\n",
      " 19  processed_tweet     object\n",
      " 20  clean_tweet         object\n",
      "dtypes: bool(1), int64(7), object(13)\n",
      "memory usage: 542.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe so far for future access\n",
    "df.to_pickle(\"playground_data/covid19_0320_0324_updated_v3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Reload Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"playground_data/covid19_0320_0324_updated_v3.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Testing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_corona(text):\n",
    "    regex = re.compile(r\"(corona[\\s]?virus)+|(corona)+\", re.I)\n",
    "    return regex.sub(\"coronavirus\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_covid(text):\n",
    "    regex = re.compile(r\"(covid[-\\s]?19)+\", re.I)\n",
    "    return regex.sub(\"covid19\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "    mytokens = parser(text)\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    return \" \".join(mytokens)\n",
    "\n",
    "def normalize_text(text):\n",
    "    PUNC = string.punctuation + \"…\" + \"–\"\n",
    "    text = replace_covid(text)\n",
    "    text = replace_corona(text)\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # put spaces between punctuation\n",
    "    punct = r\"[\" + re.escape(PUNC) + r\"]\"\n",
    "    text = re.sub(\"(?<! )(?=\" + punct + \")|(?<=\" + punct + \")(?! )\", r\" \", text)\n",
    "    # tokenize\n",
    "    text = spacy_tokenizer(text)\n",
    "    # create string without punctuation\n",
    "    text = \" \".join([word for word in text.split() if word not in PUNC and word not in STOP_WORDS])\n",
    "    # remove any extra whitespace\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "\n",
    "def clean_tweet_text(text):\n",
    "    text = unique_twitter_tokens(text)\n",
    "    text = normalize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2e60a89bd342e7bfe9d006d981272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3364618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"test_clean_tweet\"] = df[\"tweet\"].progress_apply(clean_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test_clean_tweet\"].str.contains(r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus disease covid19 update xxnum xxnum gmt xxemoji xxemoji total confirmed cases xxnum xxnum xxemoji total confirmed deaths xxnum xxnum xxemoji total number recovered patients xxnum xxnum xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(samptweet1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus covid19 business advice xxurl'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(samptweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curling time shine suck xxhashtag xxpictwit'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(subset[\"clean_tweet\"][101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample to work with 1k tweets\n",
    "subset = df.sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = \"covid-19 covid19 COVID19 Covid-19 covid 19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_covid(text):\n",
    "    regex = re.compile(r\"(covid[-\\s]?19)+\", re.I)\n",
    "    return \",\".join(x.group() for x in regex.finditer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_covid(text):\n",
    "    regex = re.compile(r\"(covid[-\\s]?19)+\", re.I)\n",
    "    return regex.sub(\"covid19\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid-19,covid19,COVID19,Covid-19,covid 19'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_covid(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19 covid19 covid19 covid19 covid19'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_covid(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid-19 covid19 covid19 covid-19 covid 19'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus disease (COVID-19) Update at 7:10 GMT  xxemoji   xxemoji  Total Confirmed cases  - 382,031   xxemoji  Total Confirmed deaths - 16,565   xxemoji  Total Number of Recovered patients - 102,481  xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag\n"
     ]
    }
   ],
   "source": [
    "samptweet1 = subset[\"clean_tweet\"].sample(n=1, random_state=9); print(samptweet1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus (COVID-19) Business Advice -  xxurl …\n"
     ]
    }
   ],
   "source": [
    "samptweet2 = subset[\"clean_tweet\"][0]; print(samptweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus disease covid19 update 710 gmt xxemoji xxemoji total confirmed cases 382031 xxemoji total confirmed deaths 16565 xxemoji total number recovered patients 102481 xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(samptweet1.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus covid19 business advice xxurl'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(samptweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[\"clean_tweet\"] = subset[\"clean_tweet\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coronavirus covid-19 business advice xxurl\n",
      "\n",
      "find latest xxhashtag information resources cta's covid-19 information page xxurl xxpictwit\n",
      "\n",
      "coronavirus disease covid-19 update 710 gmt xxemoji xxemoji total confirmed cases 382031 xxemoji total confirmed deaths 16565 xxemoji total number recovered patients 102481 xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag\n",
      "\n",
      "xxhashtag important prevent xxhashtag outbreak xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag xxhashtag\n",
      "\n",
      "hint good news italy xxemoji lombardy italy reports day decreasing hospitalizations region covid-19 coronavirus xxurl\n",
      "\n",
      "where's place you're going over? xxhashtag xxhashtag xxhashtag xxhashtag xxpictwit\n",
      "\n",
      "stand italy trying times share support italian friends colleagues friends family cari amici siamo con voi xxhashtag xxhashtag\n",
      "\n",
      "jason heyward discusses generous donation support covid-19 relief xxurl\n",
      "\n",
      "care economy biggest employers read share act save canadas main streets covid-19 xxuser xxurl xxhashtag xxhashtag xxuser xxuser xxuser xxuser xxuser xxuser xxuser\n",
      "\n",
      "trump probably thinks covid 19 college girls video co-ed 19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print(subset[\"clean_tweet\"][n] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
