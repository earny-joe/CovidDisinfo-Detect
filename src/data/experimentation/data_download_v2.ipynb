{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Purpose_ --> Edit `data_download_v1.py` to run script from inside repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setpath():\n",
    "    return Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homepath = setpath()\n",
    "#myrepopath = homepath / 'covid_disinfo_detect'\n",
    "#panacearepopath = homepath / 'thepanacealab_covid19'\n",
    "#print(f'Home path: {homepath}')\n",
    "#print(f'Path to my repo: {myrepopath}')\n",
    "#print(f'Path to cloned Panacea repo: {panacearepopath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_panacea_repo(homepath):\n",
    "    try:\n",
    "        print('Cloning repository...')\n",
    "        gitrepo = 'https://github.com/thepanacealab/covid19_twitter.git'\n",
    "        git.Repo.clone_from(gitrepo, homepath / 'thepanacealab_covid19')\n",
    "        print('Repo cloned.')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone_panacea_repo(homepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panacea_pull(panacearepopath):\n",
    "    g = git.cmd.Git(panacearepopath)\n",
    "    result = g.pull()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panacea_pull(panacearepopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "#daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "#              if x.name not in file_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_raw_folders(myrepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub repo\n",
    "    for day in daily_list:\n",
    "        if (myrepopath / 'data' / 'raw_dailies' / day).exists():\n",
    "            pass\n",
    "        else:\n",
    "            newpath = myrepopath / 'data' / 'raw_dailies' / day\n",
    "            newpath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proc_folders(myrepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub repo\n",
    "    for day in daily_list:\n",
    "        if (myrepopath / 'data' / 'processed_dailies' / day).exists():\n",
    "            pass\n",
    "        else:\n",
    "            newpath = myrepopath / 'data' / 'processed_dailies' / day\n",
    "            newpath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_folders(myrepopath, daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_data(myrepopath, panacearepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub Repo\n",
    "    for day in daily_list:\n",
    "        # create path variables to access data in Panacea repo, and path to local storage folder\n",
    "        storagepath = myrepopath / 'data' / 'raw_dailies' / day\n",
    "        datapath = panacearepopath / 'dailies' / day\n",
    "        # get list of contents within local daily storage folder \n",
    "        files = [x.name for x in storagepath.iterdir()]\n",
    "        # if txt file with that date is in daily storage folder, print confirmation\n",
    "        if f'{day}_clean-dataset.txt' in files:\n",
    "            print(f'Txt detected in {storagepath}')\n",
    "        # else read in compressed tsv file with Tweet IDs from Panacea repo & store txt file\n",
    "        # with Tweet IDs in local daily storage folder\n",
    "        else:\n",
    "            df = pd.read_csv(f'{datapath}/{day}_clean-dataset.tsv.gz',\n",
    "                             sep='\\t', usecols=['tweet_id'], compression='gzip')\n",
    "            df.to_csv(f'{storagepath}/{day}_clean-dataset.txt', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_txt_data(myrepopath, panacearepopath, daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(x.name for x in mydatapath.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_setup():\n",
    "    # set up path to current working directory & path to directory containing Panacea data\n",
    "    homepath = setpath()\n",
    "    myrepopath = homepath / 'covid_disinfo_detect'\n",
    "    panacearepopath = homepath / 'thepanacealab_covid19'\n",
    "    if myrepopath.exists():\n",
    "        pass\n",
    "    else:\n",
    "        myrepopath.mkdir()\n",
    "    # if Panacea lab folder in working directory, print confirmation, else clone the repo\n",
    "    if 'thepanacealab_covid19' in [x.name for x in homepath.iterdir()]:\n",
    "        print('Panacea Labs COVID-19 GitHub has already been cloned...')\n",
    "    else:\n",
    "        clone_panacea_repo(path)\n",
    "        \n",
    "    # pull any recent updates from Panacea Lab repo\n",
    "    pull_result = panacea_pull(panacearepopath)\n",
    "    print(pull_result)\n",
    "    # create list of daily folders located in Panacea repo (which contains data we need to access)\n",
    "    file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "    daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "                  if x.name not in file_ignore]\n",
    "    # check to see if data sub-directory exists in my repo\n",
    "    mydatapath = myrepopath / 'data'\n",
    "    if mydatapath.exists(): \n",
    "        pass\n",
    "    else:\n",
    "        mydatapath.mkdir()\n",
    "    \n",
    "    # if raw_dailies sub-folder exists make folders for raw data and get text of IDs\n",
    "    if 'raw_dailies' in list(x.name for x in mydatapath.iterdir()):\n",
    "        make_raw_folders(myrepopath, daily_list)\n",
    "        get_txt_data(myrepopath, panacearepopath, daily_list)\n",
    "    # else make raw_dailies folder, then make folders for raw data and get text of IDs\n",
    "    else:\n",
    "        mydailypath = mydatapath / 'raw_dailies'\n",
    "        mydailypath.mkdir()\n",
    "        make_raw_folders(myrepopath, daily_list)\n",
    "        get_txt_data(myrepopath, panacearepopath, daily_list)\n",
    "        \n",
    "    # check to see if processed_dailies sub-folder exists then create daily folders    \n",
    "    if 'processed_dailies' in list(x.name for x in mydatapath.iterdir()):\n",
    "        make_proc_folders(myrepopath, daily_list)\n",
    "    else:\n",
    "        myprocdailypath = mydatapath / 'processed_dailies'\n",
    "        myprocdailypath.mkdir()\n",
    "        make_proc_folders(myrepopath, daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panacea Labs COVID-19 GitHub has already been cloned...\n",
      "Already up-to-date.\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-22\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-23\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-24\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-25\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-26\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-27\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-28\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-29\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-30\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-31\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-01\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-02\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-03\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-04\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-05\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-06\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-07\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-08\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-09\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-10\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-11\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-12\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-13\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-14\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-15\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-16\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-17\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-18\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-19\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-20\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-21\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-22\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-23\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-24\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-25\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-26\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-27\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-28\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-29\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-30\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-01\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-02\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-03\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-04\n"
     ]
    }
   ],
   "source": [
    "#main_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_exists(bucket_name, source_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_file_name)\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_check(daily_list):\n",
    "    bucket_name = 'thepanacealab_covid19twitter'\n",
    "    nojson = []\n",
    "    for day in daily_list:\n",
    "        source_file_name1 = f'dailies/{day}/{day}_clean-dataset.json'\n",
    "        source_file_name2 = f'dailies/{day}/panacealab_{day}_clean-dataset.json'\n",
    "        json1_exist = blob_exists(bucket_name, source_file_name1)\n",
    "        json2_exist = blob_exists(bucket_name, source_file_name2)\n",
    "        if json1_exist or json2_exist == True:\n",
    "            pass\n",
    "        else:\n",
    "            nojson.append(day)\n",
    "    return nojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dates that ARE NOT in storage bucket\n",
    "#storage_check(daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, chunksize=10000):\n",
    "    good_columns = [\n",
    "        'created_at',\n",
    "        'entities',\n",
    "        'favorite_count',\n",
    "        'full_text',\n",
    "        'id_str',\n",
    "        'in_reply_to_screen_name',\n",
    "        'in_reply_to_status_id_str',\n",
    "        'is_quote_status',\n",
    "        'lang',\n",
    "        'retweet_count',\n",
    "        'source',\n",
    "        'user',\n",
    "        'quoted_status_id_str',\n",
    "        'quoted_status_permalink'\n",
    "    ]\n",
    "    chunks = pd.read_json(\n",
    "        filename, lines=True, chunksize=chunksize,\n",
    "        dtype={'id_str': str, 'in_reply_to_status_id_str': str, 'quoted_status_id_str': str},\n",
    "        compression='gzip'\n",
    "    )\n",
    "    df = pd.concat(chunk for chunk in chunks)[good_columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_extraction(entity, component, urls=False, user_mentions=False):\n",
    "    try:\n",
    "        if urls == True:\n",
    "            if entity[component] == []:\n",
    "                return None\n",
    "            elif entity[component] != []:\n",
    "                return ','.join([url['url'] for url in entity[component]])\n",
    "        elif user_mentions == True:\n",
    "            if entity[component] == []:\n",
    "                return None\n",
    "            elif entity[component] != []:\n",
    "                return ','.join([mention['screen_name'] for mention in entity[component]])\n",
    "        else:\n",
    "            if entity[component] == []:\n",
    "                return None\n",
    "            elif entity[component] != []:\n",
    "                return ','.join([comp['text'] for comp in entity[component]])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_extract(text):\n",
    "    try:\n",
    "        regex = re.compile(r'(?<=>).*?(?=<)', re.I)\n",
    "        return regex.search(text).group()\n",
    "    except AttributeError as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quoted_status_extract(status):\n",
    "    try:\n",
    "        return status['url']\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_panacea_data(dataframe):\n",
    "    user_components = [\n",
    "        'created_at', 'description', 'favourites_count', 'followers_count', 'friends_count',\n",
    "        'id_str', 'location', 'name', 'profile_image_url_https', 'screen_name',\n",
    "        'statuses_count', 'verified'\n",
    "    ]\n",
    "    dataframe['hashtags'] = dataframe['entities'].apply(lambda x: entity_extraction(x, 'hashtags'))\n",
    "    dataframe['symbols'] = dataframe['entities'].apply(lambda x: entity_extraction(x, 'symbols'))\n",
    "    dataframe['urls'] = dataframe['entities'].apply(lambda x: entity_extraction(x, 'urls', urls=True))\n",
    "    dataframe['user_mentions'] = dataframe['entities'].apply(lambda x: entity_extraction(x, 'user_mentions', user_mentions=True))\n",
    "    dataframe['tweet_source'] = dataframe['source'].apply(source_extract)\n",
    "    for comp in user_components:\n",
    "        dataframe[f'user_{comp}'] = dataframe['user'].apply(lambda user: user[comp])\n",
    "    dataframe['quoted_status_url'] = dataframe['quoted_status_permalink'].apply(quoted_status_extract)\n",
    "    dataframe.drop(labels=['user', 'entities', 'source', 'quoted_status_permalink'], axis=1, inplace=True)\n",
    "    dataframe.fillna('none', inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_dailies', 'raw_dailies']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mydatapath = setpath() / 'covid_disinfo_detect' / 'data'\n",
    "\n",
    "#[x.name for x in mydatapath.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-04\n",
      "/home/jupyter/covid_disinfo_detect/data/raw_dailies\n",
      "/home/jupyter/covid_disinfo_detect/data/processed_dailies\n"
     ]
    }
   ],
   "source": [
    "#myrawdatapath = setpath() / 'covid_disinfo_detect/data/raw_dailies/'\n",
    "#myprocdatapath = setpath() / 'covid_disinfo_detect/data/processed_dailies'\n",
    "#day = '2020-05-04'\n",
    "#print(day)\n",
    "#print(myrawdatapath)\n",
    "#print(myprocdatapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_wrapper(daypath, myprocdatapath, day):\n",
    "    print('Loading data...')\n",
    "    df = load_data(f'{daypath}/{day}_clean-dataset.json.gz')\n",
    "    print('Cleaning data...')\n",
    "    df = clean_panacea_data(dataframe=df)\n",
    "    print(f'Cleaned data, converting data for date {day} to pickle format...')\n",
    "    df.to_pickle(f'{myprocdatapath}/{day}/{day}_clean-dataset.pkl')\n",
    "    print(f'Transferred file to following location: {myprocdatapath / day / day}...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Cleaning data...\n",
      "Cleaned data, converting data for date 2020-05-04 to pickle format...\n",
      "Transferred file to following location: /home/jupyter/covid_disinfo_detect/data/processed_dailies/2020-05-04/2020-05-04...\n",
      "CPU times: user 8min 4s, sys: 7.6 s, total: 8min 11s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#clean_data_wrapper(myrawdatapath, myprocdatapath, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twarc_gather(myrawdatapath, myprocdatapath, daily_list):\n",
    "    for day in daily_list:\n",
    "        daypath = myrawdatapath / day\n",
    "        twarc_command = f'twarc hydrate {daypath}/{day}_clean-dataset.txt > {daypath}/{day}_clean-dataset.json'\n",
    "        gzip_command = f'gzip -k {daypath}/{day}_clean-dataset.json'\n",
    "        try:\n",
    "            print(f'Hydrating data for {day}...')\n",
    "            subprocess.call(twarc_command, shell=True)\n",
    "            print('Done gathering data via twarc, compressing JSON...')\n",
    "            subprocess.call(gzip_command, shell=True)\n",
    "            print('File compressed! Now uploading JSON file to Storage Bucket...')\n",
    "            upload_blob(\n",
    "                bucket_name='thepanacealab_covid19twitter',\n",
    "                source_file_name=f'{daypath}/{day}_clean-dataset.json',\n",
    "                destination_blob_name=f'dailies/{day}/{day}_clean-dataset.json'\n",
    "            )\n",
    "            print(f'JSON file uploaded to Storage Bucket, now removing JSON from {day} folder...')\n",
    "            filepath = daypath / f'{day}_clean-dataset.json'\n",
    "            # remove JSON file\n",
    "            filepath.unlink()\n",
    "            print(f'JSON removed from {day} folder!')\n",
    "            # clean data\n",
    "            clean_data_wrapper(daypath, myprocdatapath, day)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panacearepopath = setpath() / 'thepanacealab_covid19'\n",
    "\n",
    "# create list of daily folders located in Panacea repo (which contains data we need to access)\n",
    "#file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "#daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "#              if x.name not in file_ignore]\n",
    "#daily_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dates that ARE NOT in storage bucket\n",
    "#nojson = storage_check(daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_gather():\n",
    "    # set up path to current working directory & path to directory containing Panacea data\n",
    "    homepath = setpath()\n",
    "    myrepopath = homepath / 'covid_disinfo_detect'\n",
    "    panacearepopath = homepath / 'thepanacealab_covid19'\n",
    "    myrawdatapath =  myrepopath / 'data' / 'raw_dailies'\n",
    "    myprocdatapath = myrepopath / 'data' / 'processed_dailies'\n",
    "    # create list of daily folders located in Panacea repo (which contains data we need to access)\n",
    "    file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "    daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "                  if x.name not in file_ignore]\n",
    "    # see what daily data we do not have in storage bucket\n",
    "    nojson = storage_check(daily_list)\n",
    "    previous3 = nojson[-3:]\n",
    "    print(f'\\nTotal of {len(nojson)} folders do not contain a JSON file:\\n{nojson}\\n')\n",
    "    print(f'Gathering data for the previous 3 days without JSONs:\\n{previous3[::-1]}')\n",
    "    twarc_gather(myrawdatapath, myprocdatapath, previous3[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program():\n",
    "    main_setup()\n",
    "    main_gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panacea Labs COVID-19 GitHub has already been cloned...\n",
      "Already up-to-date.\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-22\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-23\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-24\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-25\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-26\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-27\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-28\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-29\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-30\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-03-31\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-01\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-02\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-03\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-04\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-05\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-06\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-07\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-08\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-09\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-10\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-11\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-12\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-13\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-14\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-15\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-16\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-17\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-18\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-19\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-20\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-21\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-22\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-23\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-24\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-25\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-26\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-27\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-28\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-29\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-04-30\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-01\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-02\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-03\n",
      "Txt detected in /home/jupyter/covid_disinfo_detect/data/raw_dailies/2020-05-04\n",
      "\n",
      "Total of 26 folders do not contain a JSON file:\n",
      "['2020-03-22', '2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-06', '2020-04-14', '2020-04-15', '2020-04-16', '2020-04-17', '2020-04-18', '2020-04-19', '2020-04-20', '2020-04-21', '2020-04-30', '2020-05-03', '2020-05-04']\n",
      "\n",
      "Gathering data for the previous 3 days without JSONs:\n",
      "['2020-05-04', '2020-05-03', '2020-04-30']\n",
      "Hydrating data for 2020-05-04...\n"
     ]
    }
   ],
   "source": [
    "main_program()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
