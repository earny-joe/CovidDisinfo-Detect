{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Development: Version 3 of Data Ingestion Python Script_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import git\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setpath():\n",
    "    return Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homepath = setpath()\n",
    "#homepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_panacea_repo(homepath):\n",
    "    try:\n",
    "        print('Cloning repository...')\n",
    "        gitrepo = 'https://github.com/thepanacealab/covid19_twitter.git'\n",
    "        git.Repo.clone_from(gitrepo, homepath / 'thepanacealab_covid19')\n",
    "        print('Repo cloned.')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone_panacea_repo(homepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panacea_pull(panacearepopath):\n",
    "    g = git.cmd.Git(panacearepopath)\n",
    "    result = g.pull()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#panacearepopath = setpath() / 'thepanacealab_covid19'\n",
    "#panacea_pull(panacearepopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_raw_folders(myrepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub repo\n",
    "    for day in daily_list:\n",
    "        if (myrepopath / 'data' / 'raw_dailies' / day).exists():\n",
    "            pass\n",
    "        else:\n",
    "            newpath = myrepopath / 'data' / 'raw_dailies' / day\n",
    "            newpath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proc_folders(myrepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub repo\n",
    "    for day in daily_list:\n",
    "        if (myrepopath / 'data' / 'processed_dailies' / day).exists():\n",
    "            pass\n",
    "        else:\n",
    "            newpath = myrepopath / 'data' / 'processed_dailies' / day\n",
    "            newpath.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_data(myrepopath, panacearepopath, daily_list):\n",
    "    # for day in list of daily folders from Panacea Labs GitHub Repo\n",
    "    for day in daily_list:\n",
    "        # create path variables to access data in Panacea repo, and path to local storage folder\n",
    "        storagepath = myrepopath / 'data' / 'raw_dailies' / day\n",
    "        datapath = panacearepopath / 'dailies' / day\n",
    "        # get list of contents within local daily storage folder \n",
    "        files = [x.name for x in storagepath.iterdir()]\n",
    "        # if txt file with that date is in daily storage folder, print confirmation\n",
    "        if f'{day}_clean-dataset.txt' in files:\n",
    "            pass # print(f'Txt detected in {storagepath}')\n",
    "        # else read in compressed tsv file with Tweet IDs from Panacea repo & store txt file\n",
    "        # with Tweet IDs in local daily storage folder\n",
    "        else:\n",
    "            df = pd.read_csv(f'{datapath}/{day}_clean-dataset.tsv.gz',\n",
    "                             sep='\\t', usecols=['tweet_id'], compression='gzip')\n",
    "            df.to_csv(f'{storagepath}/{day}_clean-dataset.txt', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_setup():\n",
    "    # set up path to current working directory & path to directory containing Panacea data\n",
    "    homepath = setpath()\n",
    "    myrepopath = Path.cwd().parent.parent\n",
    "    panacearepopath = homepath / 'thepanacealab_covid19'\n",
    "    if myrepopath.exists():\n",
    "        pass\n",
    "    else:\n",
    "        myrepopath.mkdir()\n",
    "    # if Panacea lab folder in working directory, print confirmation, else clone the repo\n",
    "    if 'thepanacealab_covid19' in [x.name for x in homepath.iterdir()]:\n",
    "        print('Panacea Labs COVID-19 GitHub has already been cloned...')\n",
    "    else:\n",
    "        clone_panacea_repo(path)\n",
    "        \n",
    "    # pull any recent updates from Panacea Lab repo\n",
    "    pull_result = panacea_pull(panacearepopath)\n",
    "    print(pull_result)\n",
    "    # create list of daily folders located in Panacea repo (which contains data we need to access)\n",
    "    file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "    daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "                  if x.name not in file_ignore]\n",
    "    # check to see if data sub-directory exists in my repo\n",
    "    mydatapath = myrepopath / 'data'\n",
    "    if mydatapath.exists(): \n",
    "        pass\n",
    "    else:\n",
    "        mydatapath.mkdir()\n",
    "    \n",
    "    # if raw_dailies sub-folder exists make folders for raw data and get text of IDs\n",
    "    if 'raw_dailies' in list(x.name for x in mydatapath.iterdir()):\n",
    "        make_raw_folders(myrepopath, daily_list)\n",
    "        get_txt_data(myrepopath, panacearepopath, daily_list)\n",
    "    # else make raw_dailies folder, then make folders for raw data and get text of IDs\n",
    "    else:\n",
    "        mydailypath = mydatapath / 'raw_dailies'\n",
    "        mydailypath.mkdir()\n",
    "        make_raw_folders(myrepopath, daily_list)\n",
    "        get_txt_data(myrepopath, panacearepopath, daily_list)\n",
    "        \n",
    "    # check to see if processed_dailies sub-folder exists then create daily folders    \n",
    "    if 'processed_dailies' in list(x.name for x in mydatapath.iterdir()):\n",
    "        make_proc_folders(myrepopath, daily_list)\n",
    "    else:\n",
    "        myprocdailypath = mydatapath / 'processed_dailies'\n",
    "        myprocdailypath.mkdir()\n",
    "        make_proc_folders(myrepopath, daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_exists(bucket_name, source_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_file_name)\n",
    "    return blob.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_check(daily_list):\n",
    "    bucket_name = 'thepanacealab_covid19twitter'\n",
    "    nojson = []\n",
    "    for day in daily_list:\n",
    "        source_file_name1 = f'dailies/{day}/{day}_clean-dataset.json'\n",
    "        source_file_name2 = f'dailies/{day}/panacealab_{day}_clean-dataset.json'\n",
    "        json1_exist = blob_exists(bucket_name, source_file_name1)\n",
    "        json2_exist = blob_exists(bucket_name, source_file_name2)\n",
    "        if json1_exist or json2_exist == True:\n",
    "            pass\n",
    "        else:\n",
    "            nojson.append(day)\n",
    "    return nojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "#daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "#              if x.name not in file_ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def implicit():\n",
    "#    from google.cloud import storage\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "#    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "#    buckets = list(storage_client.list_buckets())\n",
    "#    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nojson = storage_check(daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous3 = nojson[-3:]\n",
    "#print(previous3)\n",
    "#testday = nojson[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.call([\"ls\", \"-l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_output([\"ls\", \"-l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myrepopath = homepath/'Documents/SharpestMinds/covid_disinfo_detect'\n",
    "#myrawdatapath =  myrepopath/'data'/'raw_dailies'\n",
    "#print(previous3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day = '2020-05-18'\n",
    "#daypath = myrawdatapath / day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twarc_command = f'twarc hydrate {daypath}/{day}_clean-dataset.txt > {daypath}/{day}_clean-dataset.json'\n",
    "#print(twarc_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.call(twarc_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twarc_gather(myrawdatapath, daily_list):\n",
    "    #print(f'Hydrating data for the following days: {daily_list}')\n",
    "    for day in daily_list:\n",
    "        daypath = myrawdatapath / day\n",
    "        twarc_command = f'twarc hydrate {daypath}/{day}_clean-dataset.txt > {daypath}/{day}_clean-dataset.json'\n",
    "        # gzip_command = f'gzip -k {daypath}/{day}_clean-dataset.json'\n",
    "        try:\n",
    "            print(f'Hydrating data for {day}...')\n",
    "            subprocess.call(twarc_command, shell=True)\n",
    "            #print('Done gathering data via twarc, compressing JSON...')\n",
    "            #subprocess.call(gzip_command, shell=True)\n",
    "            #print('File compressed! Now uploading JSON file to Storage Bucket...')\n",
    "            print('Uploading to bucket...')\n",
    "            upload_blob(\n",
    "                bucket_name='thepanacealab_covid19twitter',\n",
    "                source_file_name=f'{daypath}/{day}_clean-dataset.json',\n",
    "                destination_blob_name=f'dailies/{day}/{day}_clean-dataset.json'\n",
    "            )\n",
    "            print(f'JSON file uploaded to Storage Bucket, now removing JSON from {day} folder...')\n",
    "            filepath = daypath / f'{day}_clean-dataset.json'\n",
    "            # remove JSON file\n",
    "            filepath.unlink()\n",
    "            print(f'JSON removed from {day} folder!')\n",
    "            # clean data --> not for use locally\n",
    "            # clean_data_wrapper(daypath, myprocdatapath, day)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twarc_gather(myrawdatapath, previous3[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#twarc_gather(myrawdatapath, previous3[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#twarc_gather(myrawdatapath, previous3[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homepath = setpath()\n",
    "#myrepopath = Path.cwd().parent.parent\n",
    "#print(homepath)\n",
    "#print(myrepopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_gather():\n",
    "    # set up path to current working directory & path to directory containing Panacea data\n",
    "    homepath = setpath()\n",
    "    myrepopath = Path.cwd().parent.parent\n",
    "    panacearepopath = homepath / 'thepanacealab_covid19'\n",
    "    myrawdatapath =  myrepopath / 'data' / 'raw_dailies'\n",
    "    #myprocdatapath = myrepopath / 'data' / 'processed_dailies' --> don't belive I need at the moment\n",
    "    # create list of daily folders located in Panacea repo (which contains data we need to access)\n",
    "    file_ignore = ['README.md', '.ipynb_checkpoints']\n",
    "    daily_list = [x.name for x in sorted((panacearepopath / 'dailies').iterdir())\\\n",
    "                  if x.name not in file_ignore]\n",
    "    # see what daily data we do not have in storage bucket\n",
    "    nojson = storage_check(daily_list)\n",
    "    #previous4 = nojson[-4:]\n",
    "    print(f'\\nTotal of {len(nojson)} folders do not contain a JSON file:\\n{nojson}\\n')\n",
    "    print(f'Gathering data for the previous days without JSONs:\\n{nojson[::-1]}')\n",
    "    twarc_gather(myrawdatapath, nojson[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_program():\n",
    "    main_setup()\n",
    "    main_gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-05-25 17:03:29.613767\n",
      "Panacea Labs COVID-19 GitHub has already been cloned...\n",
      "Already up to date.\n",
      "\n",
      "Total of 2 folders do not contain a JSON file:\n",
      "['2020-03-22', '2020-03-28']\n",
      "\n",
      "Gathering data for the previous days without JSONs:\n",
      "['2020-03-28', '2020-03-22']\n",
      "Hydrating data for 2020-03-28...\n",
      "Uploading to bucket...\n",
      "File /Users/jairesearch/Documents/SharpestMinds/covid_disinfo_detect/data/raw_dailies/2020-03-28/2020-03-28_clean-dataset.json uploaded to dailies/2020-03-28/2020-03-28_clean-dataset.json.\n",
      "JSON file uploaded to Storage Bucket, now removing JSON from 2020-03-28 folder...\n",
      "JSON removed from 2020-03-28 folder!\n",
      "Hydrating data for 2020-03-22...\n",
      "Uploading to bucket...\n",
      "File /Users/jairesearch/Documents/SharpestMinds/covid_disinfo_detect/data/raw_dailies/2020-03-22/2020-03-22_clean-dataset.json uploaded to dailies/2020-03-22/2020-03-22_clean-dataset.json.\n",
      "JSON file uploaded to Storage Bucket, now removing JSON from 2020-03-22 folder...\n",
      "JSON removed from 2020-03-22 folder!\n",
      "Ended at: 2020-05-25 22:22:43.523900\n",
      "CPU times: user 1min 14s, sys: 44.4 s, total: 1min 59s\n",
      "Wall time: 5h 19min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'Started at: {datetime.now()}')\n",
    "main_program()\n",
    "print(f'Ended at: {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
